{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b4b40130-43d3-4e9c-90a7-c3f62e7b10e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##파이썬##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a57b7976-64eb-407b-907b-3194e22fa9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF 로드 및 분할 성공\n",
      "임베딩 설정 성공\n",
      "질의응답 체인 설정 성공\n",
      "SQLChatMessageHistory 생성 성공: C:\\Users\\유승미\\AppData\\Roaming\\jupyter\\runtime\\kernel-cdbe03a7-aae3-42a6-9df3-43c41d3276e8.json\n",
      "응답 생성 성공: content='It seems like there may be a misunderstanding. Please provide more information or let me know how I can assist you.' response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 95, 'total_tokens': 118}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-d27716ac-7628-4e70-bc45-0e9f269fa783-0' usage_metadata={'input_tokens': 95, 'output_tokens': 23, 'total_tokens': 118}\n",
      "chat_response 구조: content='It seems like there may be a misunderstanding. Please provide more information or let me know how I can assist you.' response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 95, 'total_tokens': 118}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-d27716ac-7628-4e70-bc45-0e9f269fa783-0' usage_metadata={'input_tokens': 95, 'output_tokens': 23, 'total_tokens': 118}\n",
      "{\n",
      "    \"status\": 200,\n",
      "    \"message\": \"채팅 응답 성공\",\n",
      "    \"body\": {\n",
      "        \"chatMessage\": \"content='It seems like there may be a misunderstanding. Please provide more information or let me know how I can assist you.' response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 95, 'total_tokens': 118}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-d27716ac-7628-4e70-bc45-0e9f269fa783-0' usage_metadata={'input_tokens': 95, 'output_tokens': 23, 'total_tokens': 118}\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import sys\n",
    "import json\n",
    "import tiktoken\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.chat_message_histories import SQLChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# 환경 변수 설정\n",
    "os.environ[\"OPENAI_API_KEY\"] = 'sk-'\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# 토큰화 설정\n",
    "tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "def tiktoken_len(text):\n",
    "    tokens = tokenizer.encode(text)\n",
    "    return len(tokens)\n",
    "\n",
    "# PDF 로드 및 텍스트 분할\n",
    "try:\n",
    "    loader = PyPDFLoader(\"D:/[24]ICT_Practice/practice_file/(2024)포트미스_가이드북.pdf\")\n",
    "    pages = loader.load_and_split()\n",
    "    print(\"PDF 로드 및 분할 성공\")\n",
    "except Exception as e:\n",
    "    print(f\"PDF 로드 및 분할 실패: {e}\")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50, length_function=tiktoken_len)\n",
    "texts = text_splitter.split_documents(pages)\n",
    "\n",
    "# 임베딩 설정\n",
    "try:\n",
    "    model_name = \"jhgan/ko-sbert-nli\"\n",
    "    model_kwargs = {'device': 'cpu'}\n",
    "    encode_kwargs = {'normalize_embeddings': True}\n",
    "    hf = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs)\n",
    "    docsearch = Chroma.from_documents(texts, hf)\n",
    "    print(\"임베딩 설정 성공\")\n",
    "except Exception as e:\n",
    "    print(f\"임베딩 설정 실패: {e}\")\n",
    "\n",
    "# OpenAI 모델 설정\n",
    "openai_model = ChatOpenAI(model_name=\"gpt-3.5-turbo\", streaming=False, temperature=0)\n",
    "\n",
    "# SQLChatMessageHistory 객체 생성\n",
    "def get_chat_message_history(session_id):\n",
    "    try:\n",
    "        history = SQLChatMessageHistory(session_id=session_id, connection_string=\"sqlite:///sqlite.db\")\n",
    "        print(f\"SQLChatMessageHistory 생성 성공: {session_id}\")\n",
    "        return history\n",
    "    except Exception as e:\n",
    "        print(f\"SQLChatMessageHistory 생성 실패: {e}\")\n",
    "        raise e\n",
    "\n",
    "# 채팅 프롬프트 템플릿 설정\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 체인 설정\n",
    "chain = prompt | openai_model\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_chat_message_history,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"history\"\n",
    ")\n",
    "\n",
    "# 질의응답 체인 설정\n",
    "try:\n",
    "    qa = RetrievalQA.from_chain_type(\n",
    "        llm=openai_model,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=docsearch.as_retriever(search_type=\"mmr\", search_kwargs={'k': 3, 'fetch_k': 10}),\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    print(\"질의응답 체인 설정 성공\")\n",
    "except Exception as e:\n",
    "    print(f\"질의응답 체인 설정 실패: {e}\")\n",
    "\n",
    "# 사용자 입력 처리 함수\n",
    "def get_response(input_text, session_id):\n",
    "    try:\n",
    "        config = {\"configurable\": {\"session_id\": session_id}}\n",
    "        chat_response = chain_with_history.invoke({\"question\": input_text}, config=config)\n",
    "        print(f\"응답 생성 성공: {chat_response}\")\n",
    "        return chat_response\n",
    "    except Exception as e:\n",
    "        print(f\"응답 생성 실패: {e}\")\n",
    "        raise e\n",
    "\n",
    "# 메인 함수\n",
    "if __name__ == \"__main__\":\n",
    "    input_text = sys.argv[1]\n",
    "    session_id = sys.argv[2]  # 세션 ID를 명령줄 인수로 받음\n",
    "    try:\n",
    "        chat_response = get_response(input_text, session_id)\n",
    "        \n",
    "        # chat_response 구조 출력\n",
    "        print(f\"chat_response 구조: {chat_response}\")\n",
    "        \n",
    "        # chat_response의 'result'를 가져오는 부분 수정\n",
    "        if isinstance(chat_response, dict) and \"content\" in chat_response:\n",
    "            chat_message = chat_response[\"content\"]\n",
    "        else:\n",
    "            chat_message = str(chat_response)\n",
    "\n",
    "        output = {\n",
    "            \"status\": 200,\n",
    "            \"message\": \"채팅 응답 성공\",\n",
    "            \"body\": {\n",
    "                \"chatMessage\": chat_message\n",
    "            }\n",
    "        }\n",
    "    except Exception as e:\n",
    "        output = {\n",
    "            \"status\": 404,\n",
    "            \"message\": \"채팅 응답 실패\",\n",
    "            \"body\": {\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    print(json.dumps(output, ensure_ascii=False, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f20254a5-d585-44f1-9fc8-48cd03538c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import sys\n",
    "import json\n",
    "import tiktoken\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.chat_message_histories import SQLChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# 환경 변수 설정\n",
    "os.environ[\"OPENAI_API_KEY\"] = 'sk-'\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# 토큰화 설정\n",
    "tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "def tiktoken_len(text):\n",
    "    tokens = tokenizer.encode(text)\n",
    "    return len(tokens)\n",
    "\n",
    "# PDF 로드 및 텍스트 분할\n",
    "loader = PyPDFLoader(\"D:/[24]ICT_Practice/practice_file/(2024)포트미스_가이드북.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50, length_function=tiktoken_len)\n",
    "texts = text_splitter.split_documents(pages)\n",
    "\n",
    "# 임베딩 설정\n",
    "model_name = \"jhgan/ko-sbert-nli\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "hf = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs)\n",
    "docsearch = Chroma.from_documents(texts, hf)\n",
    "\n",
    "# OpenAI 모델 설정\n",
    "openai_model = ChatOpenAI(model_name=\"gpt-3.5-turbo\", streaming=False, temperature=0)\n",
    "\n",
    "# SQLChatMessageHistory 객체 생성\n",
    "def get_chat_message_history(session_id):\n",
    "    return SQLChatMessageHistory(session_id=session_id, connection_string=\"sqlite:///sqlite.db\")\n",
    "\n",
    "# 채팅 프롬프트 템플릿 설정\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 체인 설정\n",
    "chain = prompt | openai_model\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_chat_message_history,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"history\"\n",
    ")\n",
    "\n",
    "# 질의응답 체인 설정\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=openai_model,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=docsearch.as_retriever(search_type=\"mmr\", search_kwargs={'k': 3, 'fetch_k': 10}),\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# 사용자 입력 처리 함수\n",
    "def get_response(input_text, session_id):\n",
    "    config = {\"configurable\": {\"session_id\": session_id}}\n",
    "    try:\n",
    "        chat_response = chain_with_history.invoke({\"question\": input_text}, config=config)\n",
    "        if isinstance(chat_response, dict) and \"content\" in chat_response:\n",
    "            chat_message = chat_response[\"content\"]\n",
    "        else:\n",
    "            chat_message = str(chat_response)\n",
    "        \n",
    "        return {\n",
    "            \"status\": 200,\n",
    "            \"message\": \"채팅 응답 성공\",\n",
    "            \"body\": {\n",
    "                \"chatMessage\": chat_message\n",
    "            }\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"status\": 404,\n",
    "            \"message\": \"채팅 응답 실패\",\n",
    "            \"body\": {\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Jupyter Notebook에서 직접 호출하기\n",
    "def test_chat(input_text, session_id):\n",
    "    result = get_response(input_text, session_id)\n",
    "    return json.dumps(result, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ccdf17d-1eef-41af-9904-e29f231640c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"status\": 200,\n",
      "    \"message\": \"채팅 응답 성공\",\n",
      "    \"body\": {\n",
      "        \"chatMessage\": \"content='외항선 입항신고에서 선박이 입항 전에 다음과 같은 정보를 신고해야 합니다:\\\\n\\\\n1. 선박의 이름 및 국적\\\\n2. 선박의 IMO 번호 (선박 식별번호)\\\\n3. 선박의 총 톤수 및 길이\\\\n4. 선박의 운항 목적지 및 출발지\\\\n5. 선박의 ETA (예상 도착 시간)\\\\n6. 선박의 선원 수 및 승객 수\\\\n7. 선박이 운송하는 화물의 종류 및 양\\\\n8. 선박의 보안 상태 및 방역 상태\\\\n\\\\n이러한 정보는 해당 국가의 관련 규정에 따라 상세 내용이 달라질 수 있으니, 해당 규정을 확인하시는 것이 좋습니다.' response_metadata={'token_usage': {'completion_tokens': 252, 'prompt_tokens': 55, 'total_tokens': 307}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-a1286f16-d098-4416-b3a2-9dc2b3468bcc-0' usage_metadata={'input_tokens': 55, 'output_tokens': 252, 'total_tokens': 307}\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "input_text = \"외항선 입항신고에서 선박이 입항 전에 어 떤 정보를 신고해야 하는가?\"\n",
    "session_id = \"session_1\"\n",
    "result = test_chat(input_text, session_id)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "684fffa4-fc61-47db-8953-886c6da12c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"status\": 200,\n",
      "    \"message\": \"채팅 응답 성공\",\n",
      "    \"body\": {\n",
      "        \"chatMessage\": \"content='통합화물신고는 국제무역에서 수출 및 수입 화물에 대한 통관 절차를 간소화하고 효율화하기 위한 제도입니다. 일반적으로 통합화물신고 절차는 다음과 같습니다:\\\\n\\\\n1. 수출자 또는 수입자가 통관을 위해 통합화물신고를 작성합니다.\\\\n2. 통합화물신고에는 화물의 세부 정보, 운송 수단, 운송 업체, 수출자/수입자 정보 등이 포함됩니다.\\\\n3. 통합화물신고는 전자적으로 관세청 또는 관련 기관에 제출됩니다.\\\\n4. 관세청 또는 관련 기관은 제출된 통합화물신고를 검토하고 필요한 검사 및 검수를 실시합니다.\\\\n5. 검사 및 검수가 완료되면 통관이 승인되고 화물이 통관됩니다.\\\\n6. 통관이 완료되면 수출자 또는 수입자는 해당 화물을 수령하거나 발송할 수 있습니다.\\\\n\\\\n통합화물신고는 국가별로 절차와 요구사항이 다를 수 있으므로, 해당 국가의 관세청 또는 관련 기관의 지침을 따라야 합니다.' response_metadata={'token_usage': {'completion_tokens': 419, 'prompt_tokens': 340, 'total_tokens': 759}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-de5d6d8f-e296-4a3b-af63-946304b25082-0' usage_metadata={'input_tokens': 340, 'output_tokens': 419, 'total_tokens': 759}\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "input_text = \"통합화물신고 절차는 어떻게 되나요?\"\n",
    "session_id = \"session_1\"\n",
    "result = test_chat(input_text, session_id)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7217c8b2-83bf-4b34-8ab9-dd1003a19a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import sys\n",
    "import json\n",
    "import tiktoken\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.chat_message_histories import SQLChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# 환경 변수 설정\n",
    "os.environ[\"OPENAI_API_KEY\"] = 'sk-proj-1r91qhM6nM2lAvStku8vT3BlbkFJKzoqJnRd41ZRBojOQVLl'\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# 토큰화 설정\n",
    "tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "def tiktoken_len(text):\n",
    "    tokens = tokenizer.encode(text)\n",
    "    return len(tokens)\n",
    "\n",
    "# PDF 로드 및 텍스트 분할\n",
    "loader = PyPDFLoader(\"D:/[24]ICT_Practice/practice_file/(2024)포트미스_가이드북.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50, length_function=tiktoken_len)\n",
    "texts = text_splitter.split_documents(pages)\n",
    "\n",
    "# 임베딩 설정\n",
    "model_name = \"jhgan/ko-sbert-nli\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "hf = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs)\n",
    "docsearch = Chroma.from_documents(texts, hf)\n",
    "\n",
    "# OpenAI 모델 설정\n",
    "openai_model = ChatOpenAI(model_name=\"gpt-3.5-turbo\", streaming=False, temperature=0)\n",
    "\n",
    "# SQLChatMessageHistory 객체 생성\n",
    "def get_chat_message_history(session_id):\n",
    "    return SQLChatMessageHistory(session_id=session_id, connection_string=\"sqlite:///sqlite.db\")\n",
    "\n",
    "# 채팅 프롬프트 템플릿 설정\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 체인 설정\n",
    "chain = prompt | openai_model\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_chat_message_history,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"history\"\n",
    ")\n",
    "\n",
    "# 질의응답 체인 설정\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=openai_model,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=docsearch.as_retriever(search_type=\"mmr\", search_kwargs={'k': 3, 'fetch_k': 10}),\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# 사용자 입력 처리 함수\n",
    "def get_response(input_text, session_id):\n",
    "    config = {\"configurable\": {\"session_id\": session_id}}\n",
    "    try:\n",
    "        chat_response = chain_with_history.invoke({\"question\": input_text}, config=config)\n",
    "        if isinstance(chat_response, dict) and \"content\" in chat_response:\n",
    "            chat_message = chat_response[\"content\"]\n",
    "        else:\n",
    "            chat_message = str(chat_response)\n",
    "        \n",
    "        return {\n",
    "            \"status\": 200,\n",
    "            \"message\": \"채팅 응답 성공\",\n",
    "            \"body\": {\n",
    "                \"chatMessage\": chat_message\n",
    "            }\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"status\": 404,\n",
    "            \"message\": \"채팅 응답 실패\",\n",
    "            \"body\": {\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "74a4b9ea-8872-4c15-99d2-c288820f6043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 채팅 기록 조회 함수\n",
    "def get_chat_history(session_id):\n",
    "    history = get_chat_message_history(session_id)\n",
    "    messages = history.load_messages()\n",
    "    return messages\n",
    "\n",
    "# 특정 질문 찾기\n",
    "def find_question(session_id, question_text):\n",
    "    messages = get_chat_history(session_id)\n",
    "    for message in messages:\n",
    "        if message.role == \"human\" and question_text in message.content:\n",
    "            return message.content\n",
    "    return None\n",
    "\n",
    "# Jupyter Notebook에서 직접 호출하기\n",
    "def test_chat(input_text, session_id):\n",
    "    result = get_response(input_text, session_id)\n",
    "    return json.dumps(result, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fdfbd2a9-394e-483e-8c51-5b33cd1642f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"status\": 200,\n",
      "    \"message\": \"채팅 응답 성공\",\n",
      "    \"body\": {\n",
      "        \"chatMessage\": \"content='외항선 입항신고에서 선박이 입항 전에 다음과 같은 정보를 신고해야 합니다:\\\\n\\\\n1. 선박의 이름 및 국적\\\\n2. 선박의 IMO 번호 (선박 식별번호)\\\\n3. 선박의 총 톤수 및 길이\\\\n4. 선박의 운항 목적지 및 출발지\\\\n5. 선박의 ETA (예상 도착 시간)\\\\n6. 선박의 선원 수 및 승객 수\\\\n7. 선박이 운송하는 화물의 종류 및 양\\\\n8. 선박의 보안 상태 및 방역 상태\\\\n\\\\n이러한 정보는 해당 국가의 관련 규정에 따라 상세 내용이 달라질 수 있으니, 해당 규정을 확인하시는 것이 좋습니다.' response_metadata={'token_usage': {'completion_tokens': 252, 'prompt_tokens': 805, 'total_tokens': 1057}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-4a56d8d7-5de0-4ac2-913e-fb83475f6f18-0' usage_metadata={'input_tokens': 805, 'output_tokens': 252, 'total_tokens': 1057}\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# 예제 실행\n",
    "input_text = \"외항선 입항신고에서 선박이 입항 전에 어 떤 정보를 신고해야 하는가?\"\n",
    "session_id = \"session_1\"\n",
    "result = test_chat(input_text, session_id)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0a270d87-1a4f-4a71-a4e4-d19f8478537f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SQLChatMessageHistory' object has no attribute 'load_messages'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 질문 내용 찾기 예제 실행\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m found_question \u001b[38;5;241m=\u001b[39m \u001b[43mfind_question\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m외항선 입항신고에서 선박이 입항 전에\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(found_question)\n",
      "Cell \u001b[1;32mIn[28], line 9\u001b[0m, in \u001b[0;36mfind_question\u001b[1;34m(session_id, question_text)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_question\u001b[39m(session_id, question_text):\n\u001b[1;32m----> 9\u001b[0m     messages \u001b[38;5;241m=\u001b[39m \u001b[43mget_chat_history\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m messages:\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m message\u001b[38;5;241m.\u001b[39mrole \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m question_text \u001b[38;5;129;01min\u001b[39;00m message\u001b[38;5;241m.\u001b[39mcontent:\n",
      "Cell \u001b[1;32mIn[28], line 4\u001b[0m, in \u001b[0;36mget_chat_history\u001b[1;34m(session_id)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_chat_history\u001b[39m(session_id):\n\u001b[0;32m      3\u001b[0m     history \u001b[38;5;241m=\u001b[39m get_chat_message_history(session_id)\n\u001b[1;32m----> 4\u001b[0m     messages \u001b[38;5;241m=\u001b[39m \u001b[43mhistory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_messages\u001b[49m()\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m messages\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SQLChatMessageHistory' object has no attribute 'load_messages'"
     ]
    }
   ],
   "source": [
    "# 질문 내용 찾기 예제 실행\n",
    "found_question = find_question(session_id, \"외항선 입항신고에서 선박이 입항 전에\")\n",
    "print(found_question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchainenv",
   "language": "python",
   "name": "langchainenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
