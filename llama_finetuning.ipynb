{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM4syL4mASYcYF2ROkf+icx"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "테디 노트(02_Unsloth_llama3_파인튜닝_alpaca) 실습"
      ],
      "metadata": {
        "id": "85sx_3uMgyby"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/teddylee777/langchain-kr/blob/main/15-FineTuning/02_Unsloth_llama3_%ED%8C%8C%EC%9D%B8%ED%8A%9C%EB%8B%9D_alpaca.ipynb"
      ],
      "metadata": {
        "id": "R8CL9T-RgrRg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "torch.cuda.get_device_capability() 함수를 사용하여 현재 CUDA 장치의 major 버전과 minor 버전을 조회"
      ],
      "metadata": {
        "id": "3s0K-2169IuJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8MBc7Ca8p_d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# CUDA 장치의 주요 버전과 부 버전을 가져옵니다.\n",
        "major_version, minor_version = torch.cuda.get_device_capability()\n",
        "major_version, minor_version"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*unsloth* 라이브러리와 관련 디펜던시를 설치하는 과정\n",
        "\n",
        "* Colab 환경에서 torch 버전 2.2.1과 호환되지 않는 패키지를 회피하기 위해 unsloth 라이브러리를 별도로 설치합니다.\n",
        "* GPU의 종류(신형 또는 구형)에 따라 조건부로 필요한 패키지들을 설치합니다.\n",
        "    * 신형 GPU(Ampere, Hopper 등)의 경우, packaging, ninja, einops, flash-attn, xformers, trl, peft, accelerate, bitsandbytes 패키지를 의존성 없이 설치합니다.\n",
        "    * 구형 GPU(V100, Tesla T4, RTX 20xx 등)의 경우, xformers, trl, peft, accelerate, bitsandbytes 패키지를 의존성 없이 설치합니다.\n",
        "*설치 과정에서 발생하는 출력을 숨기기 위해 %%capture 매직 커맨드를 사용합니다."
      ],
      "metadata": {
        "id": "PxWPV0uA9PdY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "%%capture\n",
        "# Colab에서 torch 2.2.1을 사용하고 있으므로, 패키지 충돌을 방지하기 위해 별도로 설치해야 합니다.\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "if major_version >= 8:\n",
        "    # 새로운 GPU(예: Ampere, Hopper GPUs - RTX 30xx, RTX 40xx, A100, H100, L40)에 사용하세요.\n",
        "    !pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes\n",
        "else:\n",
        "    # 오래된 GPU(예: V100, Tesla T4, RTX 20xx)에 사용하세요.\n",
        "    !pip install --no-deps xformers trl peft accelerate bitsandbytes\n",
        "pass\n",
        "'''"
      ],
      "metadata": {
        "id": "1hEqbb0K83RQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "아래 코드로 수정 ↓"
      ],
      "metadata": {
        "id": "XlWj55zJENfm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "gja03P_YCTo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Unsloth**\n",
        "* Unsloth는 Llama, Mistral, CodeLlama, TinyLlama, Vicuna, Open Hermes 등을 지원합니다. 그리고 Yi, Qwen(llamafied), Deepseek, 모든 Llama, Mistral 파생 아키텍처도 지원합니다.\n",
        "\n",
        "* Unsloth는 16비트 LoRA 또는 4비트 QLoRA를 지원합니다. 둘 다 2배 빠릅니다.\n",
        "\n",
        "* max_seq_length는 kaiokendev의 방법을 통해 자동으로 RoPE 스케일링을 하기 때문에 어떤 값으로도 설정할 수 있습니다."
      ],
      "metadata": {
        "id": "r5UEtkad9fWR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "FastLanguageModel.from_pretrained 함수를 사용하여 사전 훈련된 언어 모델을 로드하는 과정을 설명\n",
        "\n",
        "* 최대 시퀀스 길이(max_seq_length)를 설정하여 모델이 처리할 수 있는 입력 데이터의 길이를 지정합니다.\n",
        "* 데이터 타입(dtype)은 자동 감지되거나, 특정 하드웨어에 최적화된 형식(Float16, Bfloat16)으로 설정할 수 있습니다.\n",
        "* 4비트 양자화(load_in_4bit) 옵션을 사용하여 메모리 사용량을 줄일 수 있으며, 이는 선택적입니다.\n",
        "사전 정의된 4비트 양자화 모델 목록(fourbit_models)에서 선택하여 다운로드 시간을 단축하고 메모리 부족 문제를 방지할 수 있습니다.\n",
        "* FastLanguageModel.from_pretrained 함수를 통해 모델과 토크나이저를 로드하며, 이때 모델 이름(model_name), 최대 시퀀스 길이, 데이터 타입, 4비트 로딩 여부를 매개변수로 전달합니다.\n",
        "* 선택적으로, 특정 게이트 모델을 사용할 경우 토큰(token)을 제공할 수 있습니다."
      ],
      "metadata": {
        "id": "sV1ZcARt9sWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "max_seq_length = 4096  # 최대 시퀀스 길이를 설정합니다. 내부적으로 RoPE 스케일링을 자동으로 지원합니다!\n",
        "# 자동 감지를 위해 None을 사용합니다. Tesla T4, V100은 Float16, Ampere+는 Bfloat16을 사용하세요.\n",
        "dtype = None\n",
        "# 메모리 사용량을 줄이기 위해 4bit 양자화를 사용합니다. False일 수도 있습니다.\n",
        "load_in_4bit = True\n",
        "\n",
        "# 4배 빠른 다운로드와 메모리 부족 문제를 방지하기 위해 지원하는 4bit 사전 양자화 모델입니다.\n",
        "fourbit_models = [\n",
        "    \"unsloth/mistral-7b-bnb-4bit\",\n",
        "    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
        "    \"unsloth/llama-2-7b-bnb-4bit\",\n",
        "    \"unsloth/gemma-7b-bnb-4bit\",\n",
        "    \"unsloth/gemma-7b-it-bnb-4bit\",  # Gemma 7b의 Instruct 버전\n",
        "    \"unsloth/gemma-2b-bnb-4bit\",\n",
        "    \"unsloth/gemma-2b-it-bnb-4bit\",  # Gemma 2b의 Instruct 버전\n",
        "    \"unsloth/llama-3-8b-bnb-4bit\",  # Llama-3 8B\n",
        "]  # 더 많은 모델은 https://huggingface.co/unsloth 에서 확인할 수 있습니다.\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    # model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n",
        "    model_name=\"beomi/Llama-3-Open-Ko-8B-Instruct-preview\",  # 모델 이름을 설정합니다.\n",
        "    max_seq_length=max_seq_length,  # 최대 시퀀스 길이를 설정합니다.\n",
        "    dtype=dtype,  # 데이터 타입을 설정합니다.\n",
        "    load_in_4bit=load_in_4bit,  # 4bit 양자화 로드 여부를 설정합니다.\n",
        "    # token = \"hf_...\", # 게이트된 모델을 사용하는 경우 토큰을 사용하세요. 예: meta-llama/Llama-2-7b-hf\n",
        ")"
      ],
      "metadata": {
        "id": "7P5qQquI9uEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- LoRA 어댑터를 추가하여 모든 파라미터 중 1% ~ 10%의 파라미터만 업데이트"
      ],
      "metadata": {
        "id": "tKF4NCw_EXSs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "FastLanguageModel을 사용하여 특정 모듈에 대한 성능 향상 기법을 적용한 모델을 구성\n",
        "\n",
        "* FastLanguageModel.get_peft_model 함수를 호출하여 모델을 초기화하고, 성능 향상을 위한 여러 파라미터를 설정합니다.\n",
        "* r 파라미터를 통해 성능 향상 기법의 강도를 조절합니다. 권장 값으로는 8, 16, 32, 64, 128 등이 있습니다.\n",
        "* target_modules 리스트에는 성능 향상을 적용할 모델의 모듈 이름들이 포함됩니다.\n",
        "* lora_alpha와 lora_dropout을 설정하여 LoRA(Low-Rank Adaptation) 기법의 세부 파라미터를 조정합니다.\n",
        "* bias 옵션을 통해 모델의 바이어스 사용 여부를 설정할 수 있으며, 최적화를 위해 \"none\"으로 설정하는 것이 권장됩니다.\n",
        "* use_gradient_checkpointing 옵션을 \"unsloth\"로 설정하여 VRAM 사용량을 줄이고, 더 큰 배치 크기로 학습할 수 있도록 합니다.\n",
        "* use_rslora 옵션을 통해 Rank Stabilized LoRA를 사용할지 여부를 결정합니다."
      ],
      "metadata": {
        "id": "2C0v3CwyEeQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,  # 0보다 큰 어떤 숫자도 선택 가능! 8, 16, 32, 64, 128이 권장됩니다.\n",
        "    lora_alpha=32,  # LoRA 알파 값을 설정합니다.\n",
        "    lora_dropout=0.05,  # 드롭아웃을 지원합니다.\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "    ],  # 타겟 모듈을 지정합니다.\n",
        "    bias=\"none\",  # 바이어스를 지원합니다.\n",
        "    # True 또는 \"unsloth\"를 사용하여 매우 긴 컨텍스트에 대해 VRAM을 30% 덜 사용하고, 2배 더 큰 배치 크기를 지원합니다.\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=123,  # 난수 상태를 설정합니다.\n",
        "    use_rslora=False,  # 순위 안정화 LoRA를 지원합니다.\n",
        "    loftq_config=None,  # LoftQ를 지원합니다.\n",
        ")"
      ],
      "metadata": {
        "id": "3oGVRm6NEcIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 준비"
      ],
      "metadata": {
        "id": "FkV3ddVXEyI0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[중요]**\n",
        "\n",
        "* 토큰화된 출력에 EOS_TOKEN을 추가하기! 그렇지 않으면 무한 생성이 발생할 수 있음."
      ],
      "metadata": {
        "id": "JMIhreRxE1tt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[참고]\n",
        "\n",
        "* 오직 완성된 텍스트만을 학습하고자 한다면, TRL의 문서를 여기에서 확인하세요."
      ],
      "metadata": {
        "id": "BoUIcF2_FLVK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "load_dataset 함수를 사용하여 특정 데이터셋을 로드하고, 이를 특정 형식으로 포매팅하는 과정을 설명합니다.\n",
        "\n",
        "* load_dataset 함수로 \"teddylee777/QA-Dataset-mini\" 데이터셋을 \"train\" 분할로 로드합니다.\n",
        "* 데이터셋의 각 예제에 대해 formatting_prompts_func 함수를 적용하여 포매팅을 수행합니다.\n",
        "* 이 함수는 \"instruction\"과 \"output\" 필드를 사용하여 주어진 포맷에 맞게 텍스트를 재구성합니다.\n",
        "* 재구성된 텍스트는 alpaca_prompt 포맷을 따르며, 각 항목의 끝에는 EOS_TOKEN을 추가하여 생성이 종료되도록 합니다.\n",
        "* 최종적으로, 포매팅된 텍스트는 \"text\" 키를 가진 딕셔너리 형태로 반환됩니다.\n",
        "* 이 과정을 통해, AI 모델이 처리하기 적합한 형태로 데이터를 전처리하는 방법을 보여줍니다."
      ],
      "metadata": {
        "id": "XBh4iU3oFCc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# EOS_TOKEN은 문장의 끝을 나타내는 토큰입니다. 이 토큰을 추가해야 합니다.\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "# AlpacaPrompt를 사용하여 지시사항을 포맷팅하는 함수입니다.\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "\n",
        "# 주어진 예시들을 포맷팅하는 함수입니다.\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]  # 지시사항을 가져옵니다.\n",
        "    outputs = examples[\"output\"]  # 출력값을 가져옵니다.\n",
        "    texts = []  # 포맷팅된 텍스트를 저장할 리스트입니다.\n",
        "    for instruction, output in zip(instructions, outputs):\n",
        "        # EOS_TOKEN을 추가해야 합니다. 그렇지 않으면 생성이 무한히 진행될 수 있습니다.\n",
        "        text = alpaca_prompt.format(instruction, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return {\n",
        "        \"text\": texts,  # 포맷팅된 텍스트를 반환합니다.\n",
        "    }\n",
        "\n",
        "\n",
        "# \"teddylee777/QA-Dataset-mini\" 데이터셋을 불러옵니다. 훈련 데이터만 사용합니다.\n",
        "dataset = load_dataset(\"teddylee777/QA-Dataset-mini\", split=\"train\")\n",
        "\n",
        "# 데이터셋에 formatting_prompts_func 함수를 적용합니다. 배치 처리를 활성화합니다.\n",
        "dataset = dataset.map(\n",
        "    formatting_prompts_func,\n",
        "    batched=True,\n",
        ")"
      ],
      "metadata": {
        "id": "rA9ToM8_FKYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델 훈련하기"
      ],
      "metadata": {
        "id": "rgNLfFg0FWaY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Huggingface TRL의 SFTTrainer를 사용\n",
        "* 참고 문서: [TRL SFT 문서](https://huggingface.co/docs/trl/sft_trainer)"
      ],
      "metadata": {
        "id": "3rHFC-XtFZCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "tokenizer.padding_side = \"right\"  # 토크나이저의 패딩을 오른쪽으로 설정합니다.\n",
        "\n",
        "# SFTTrainer를 사용하여 모델 학습 설정\n",
        "trainer = SFTTrainer(\n",
        "    model=model,  # 학습할 모델\n",
        "    tokenizer=tokenizer,  # 토크나이저\n",
        "    train_dataset=dataset,  # 학습 데이터셋\n",
        "    eval_dataset=dataset,\n",
        "    dataset_text_field=\"text\",  # 데이터셋에서 텍스트 필드의 이름\n",
        "    max_seq_length=max_seq_length,  # 최대 시퀀스 길이\n",
        "    dataset_num_proc=2,  # 데이터 처리에 사용할 프로세스 수\n",
        "    packing=False,  # 짧은 시퀀스에 대한 학습 속도를 5배 빠르게 할 수 있음\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=2,  # 각 디바이스당 훈련 배치 크기\n",
        "        gradient_accumulation_steps=4,  # 그래디언트 누적 단계\n",
        "        warmup_steps=5,  # 웜업 스텝 수\n",
        "        num_train_epochs=3,  # 훈련 에폭 수\n",
        "        max_steps=100,  # 최대 스텝 수\n",
        "        do_eval=True,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        logging_steps=1,  # logging 스텝 수\n",
        "        learning_rate=2e-4,  # 학습률\n",
        "        fp16=not torch.cuda.is_bf16_supported(),  # fp16 사용 여부, bf16이 지원되지 않는 경우에만 사용\n",
        "        bf16=torch.cuda.is_bf16_supported(),  # bf16 사용 여부, bf16이 지원되는 경우에만 사용\n",
        "        optim=\"adamw_8bit\",  # 최적화 알고리즘\n",
        "        weight_decay=0.01,  # 가중치 감소\n",
        "        lr_scheduler_type=\"cosine\",  # 학습률 스케줄러 유형\n",
        "        seed=123,  # 랜덤 시드\n",
        "        output_dir=\"outputs\",  # 출력 디렉토리\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "Yj3AS-k4FRqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* GPU의 현재 메모리 상태를 확인합니다.\n",
        "* torch.cuda.get_device_properties(0)를 사용하여 첫 번째 GPU의 속성을 조회합니다.\n",
        "* torch.cuda.max_memory_reserved()를 통해 현재 예약된 최대 메모리를 GB 단위로 계산합니다.\n",
        "* GPU의 총 메모리 크기를 GB 단위로 계산합니다.\n",
        "* GPU 이름과 최대 메모리 크기, 현재 예약된 메모리 크기를 출력합니다."
      ],
      "metadata": {
        "id": "KUC3j8saF0ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 현재 메모리 상태를 보여주는 코드\n",
        "gpu_stats = torch.cuda.get_device_properties(0)  # GPU 속성 가져오기\n",
        "start_gpu_memory = round(\n",
        "    torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3\n",
        ")  # 시작 시 예약된 GPU 메모리 계산\n",
        "max_memory = round(\n",
        "    gpu_stats.total_memory / 1024 / 1024 / 1024, 3\n",
        ")  # GPU의 최대 메모리 계산\n",
        "print(\n",
        "    f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\"\n",
        ")  # GPU 이름과 최대 메모리 출력\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")  # 예약된 메모리 양 출력"
      ],
      "metadata": {
        "id": "mExHyB-_Fz3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()  # 모델을 훈련시키고 통계를 반환합니다."
      ],
      "metadata": {
        "id": "j_xFQTfCF37x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch를 사용하여 모델 훈련 시 메모리 사용량과 훈련 시간을 계산하고 출력하는 코드입니다.\n",
        "\n",
        "* torch.cuda.max_memory_reserved()를 사용하여 훈련 중에 예약된 최대 GPU 메모리를 계산합니다.\n",
        "* 훈련 시작 시점의 GPU 메모리 사용량과 비교하여 LoRA(Low-Rank Adaptation)를 위해 사용된 추가 메모리 양을 계산합니다.\n",
        "* 전체 GPU 메모리 대비 사용된 메모리의 비율을 계산합니다.\n",
        "* 훈련에 소요된 총 시간을 초와 분 단위로 출력합니다.\n",
        "* 예약된 최대 메모리, LoRA를 위해 사용된 메모리, 그리고 이들이 전체 GPU 메모리 대비 차지하는 비율을 출력합니다."
      ],
      "metadata": {
        "id": "VMB0aByAJM6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 최종 메모리 및 시간 통계를 보여줍니다.\n",
        "used_memory = round(\n",
        "    torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3\n",
        ")  # 사용된 최대 메모리를 GB 단위로 계산합니다.\n",
        "used_memory_for_lora = round(\n",
        "    used_memory - start_gpu_memory, 3\n",
        ")  # LoRA를 위해 사용된 메모리를 GB 단위로 계산합니다.\n",
        "used_percentage = round(\n",
        "    used_memory / max_memory * 100, 3\n",
        ")  # 최대 메모리 대비 사용된 메모리의 비율을 계산합니다.\n",
        "lora_percentage = round(\n",
        "    used_memory_for_lora / max_memory * 100, 3\n",
        ")  # 최대 메모리 대비 LoRA를 위해 사용된 메모리의 비율을 계산합니다.\n",
        "print(\n",
        "    f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\"\n",
        ")  # 훈련에 사용된 시간을 초 단위로 출력합니다.\n",
        "print(\n",
        "    # 훈련에 사용된 시간을 분 단위로 출력합니다.\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(\n",
        "    f\"Peak reserved memory = {used_memory} GB.\"\n",
        ")  # 예약된 최대 메모리를 GB 단위로 출력합니다.\n",
        "print(\n",
        "    f\"Peak reserved memory for training = {used_memory_for_lora} GB.\"\n",
        ")  # 훈련을 위해 예약된 최대 메모리를 GB 단위로 출력합니다.\n",
        "print(\n",
        "    f\"Peak reserved memory % of max memory = {used_percentage} %.\"\n",
        ")  # 최대 메모리 대비 예약된 메모리의 비율을 출력합니다.\n",
        "print(\n",
        "    f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\"\n",
        ")  # 최대 메모리 대비 훈련을 위해 예약된 메모리의 비율을 출력합니다."
      ],
      "metadata": {
        "id": "Hh1gIeHPGTt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 추론"
      ],
      "metadata": {
        "id": "S7yGy3VCJUQy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델 실행. 지시사항과 입력값을 변경할 수 있으며, 출력값은 비워두기"
      ],
      "metadata": {
        "id": "n7F-4GfcJbAo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TextStreamer를 사용하여 연속적인 추론을 수행할 수도 있습니다. 이를 통해 전체를 기다리는 대신 토큰별로 생성 결과를 확인할 수 있습니다.\n",
        "\n",
        "* FastLanguageModel.for_inference(model)을 호출하여 모델의 추론 속도를 2배 향상시킵니다.\n",
        "* tokenizer를 사용하여 특정 포맷의 프롬프트를 토큰화하고, 이를 CUDA 기반의 텐서로 변환합니다. 이 과정에서 피보나치 수열을 계속하는 지시문, 입력값, 그리고 출력값을 위한 빈 공간을 포함합니다.\n",
        "* TextStreamer 객체를 tokenizer와 함께 초기화하여 텍스트 스트리밍 기능을 활성화합니다.\n",
        "* model.generate 함수를 호출하여 주어진 입력에 대한 텍스트 생성을 수행합니다. 이때, 최대 128개의 새로운 토큰을 생성할 수 있도록 설정하고, TextStreamer를 사용하여 결과를 스트리밍합니다."
      ],
      "metadata": {
        "id": "bBwgnDkmJdfS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "StoppingCriteria와 StoppingCriteriaList를 사용하여 특정 토큰에서 생성을 중단하는 방법을 구현합니다.\n",
        "\n",
        "* StopOnToken 클래스는 StoppingCriteria를 상속받아, 생성 중 특정 토큰(stop_token_id)이 나타나면 생성을 중단하도록 합니다.\n",
        "* stop_token 변수에 중단할 토큰을 문자열로 지정합니다.\n",
        "* tokenizer.encode 메소드를 사용하여 stop_token을 해당 언어 모델의 토큰 ID로 변환합니다.\n",
        "* StoppingCriteriaList에 StopOnToken 인스턴스를 포함시켜, 생성 과정에서 이를 중단 조건으로 사용합니다."
      ],
      "metadata": {
        "id": "qVC-3sVpJkAk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import StoppingCriteria, StoppingCriteriaList\n",
        "\n",
        "\n",
        "class StopOnToken(StoppingCriteria):\n",
        "    def __init__(self, stop_token_id):\n",
        "        self.stop_token_id = stop_token_id  # 정지 토큰 ID를 초기화합니다.\n",
        "\n",
        "    def __call__(self, input_ids, scores, **kwargs):\n",
        "        return (\n",
        "            self.stop_token_id in input_ids[0]\n",
        "        )  # 입력된 ID 중 정지 토큰 ID가 있으면 정지합니다.\n",
        "\n",
        "\n",
        "# end_token을 설정\n",
        "stop_token = \"<|end_of_text|>\"  # end_token으로 사용할 토큰을 설정합니다.\n",
        "stop_token_id = tokenizer.encode(stop_token, add_special_tokens=False)[\n",
        "    0\n",
        "]  # end_token의 ID를 인코딩합니다.\n",
        "\n",
        "# Stopping criteria 설정\n",
        "stopping_criteria = StoppingCriteriaList(\n",
        "    [StopOnToken(stop_token_id)]\n",
        ")  # 정지 조건을 설정합니다."
      ],
      "metadata": {
        "id": "6gKAnkLYJbUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 예시1"
      ],
      "metadata": {
        "id": "h5fnLftjJo58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextStreamer\n",
        "\n",
        "# FastLanguageModel을 이용하여 추론 속도를 2배 빠르게 설정합니다.\n",
        "FastLanguageModel.for_inference(model)\n",
        "inputs = tokenizer(\n",
        "    [\n",
        "        alpaca_prompt.format(\n",
        "            \"테디노트 유튜브 채널에 대해 알려주세요.\",  # 지시사항\n",
        "            \"\",  # 출력 - 생성을 위해 이 부분을 비워둡니다!\n",
        "        )\n",
        "    ],\n",
        "    return_tensors=\"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(\n",
        "    **inputs,\n",
        "    streamer=text_streamer,\n",
        "    max_new_tokens=4096,  # 최대 생성 토큰 수를 설정합니다.\n",
        "    stopping_criteria=stopping_criteria  # 생성을 멈출 기준을 설정합니다.\n",
        ")"
      ],
      "metadata": {
        "id": "Hi9cuaSIJnoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 예시2"
      ],
      "metadata": {
        "id": "NKdqtA-AJwMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(\n",
        "    [\n",
        "        alpaca_prompt.format(\n",
        "            \"랭체인 튜토리얼 공부할만한 사이트는?\",  # 지시사항\n",
        "            \"\",  # 출력 - 생성을 위해 이 부분을 비워둡니다!\n",
        "        )\n",
        "    ],\n",
        "    return_tensors=\"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(\n",
        "    **inputs,\n",
        "    streamer=text_streamer,\n",
        "    max_new_tokens=4096,  # 최대 생성 토큰 수를 설정합니다.\n",
        "    stopping_criteria=stopping_criteria  # 생성을 멈출 기준을 설정합니다.\n",
        ")"
      ],
      "metadata": {
        "id": "hMLlnrRlJsko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 예시3"
      ],
      "metadata": {
        "id": "2Dwv2yHoJ0oT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(\n",
        "    [\n",
        "        alpaca_prompt.format(\n",
        "            \"피보나치 수열을 이어가세요.(최대 10개)\",  # 지시사항\n",
        "            \"1, 1, 2, 3, 5, 8\",  # 출력 - 앞부분의 힌트 제공 예시\n",
        "        )\n",
        "    ],\n",
        "    return_tensors=\"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(\n",
        "    **inputs,\n",
        "    streamer=text_streamer,\n",
        "    max_new_tokens=4096,  # 최대 생성 토큰 수를 설정합니다.\n",
        "    stopping_criteria=stopping_criteria  # 생성을 멈출 기준을 설정합니다.\n",
        ")"
      ],
      "metadata": {
        "id": "ptPiDELQJx2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 저장"
      ],
      "metadata": {
        "id": "M9VCK2VtJ940"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"Llama-3-Open-Ko-8B-teddynote\")  # 모델을 로컬에 저장합니다.\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # 모델을 온라인 허브에 저장합니다."
      ],
      "metadata": {
        "id": "Gp4k_ouGJ2D0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "만약 우리가 저장한 LoRA 어댑터를 추론을 위해 불러오고 싶다면, False를 True로 설정"
      ],
      "metadata": {
        "id": "7-xA-TyAKFc-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VLLM을 위한 float16 저장"
      ],
      "metadata": {
        "id": "48X6L8HWKH27"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "우리는 float16으로 직접 저장하는 것을 지원합니다.</br>\n",
        "float16을 위해서는 merged_16bit를 선택하거나, int4를 위해서는 merged_4bit를 선택하세요. </br>\n",
        "또한, 대체 방안으로 lora 어댑터를 사용할 수 있습니다."
      ],
      "metadata": {
        "id": "BX0c5636KJ0j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델을 저장하고 Hugging Face Hub에 푸시하는 다양한 방법을 보여주는 코드입니다.\n",
        "\n",
        "* 16비트와 4비트 병합 방식으로 모델을 저장하고 푸시하는 조건문이 있으나, 이들은 실행되지 않도록 설정되어 있습니다.\n",
        "* model.save_pretrained_merged 함수와 model.push_to_hub_merged 함수를 사용하여, \"beomi/Llama-3-Open-Ko-8B\" 모델을 \"merged_4bit_forced\" 방식으로 저장하고, \"teddylee777/Llama-3-Open-Ko-8B-teddynote\"로 Hugging Face Hub에 푸시합니다.\n",
        "* LoRA 어댑터를 사용하여 모델을 저장하고 푸시하는 코드도 포함되어 있으나, 이 또한 실행되지 않도록 설정되어 있습니다.\n",
        "* 모든 저장 및 푸시 작업에는 tokenizer와 특정 save_method가 필요하며, 푸시 작업에는 추가적으로 token이 필요합니다."
      ],
      "metadata": {
        "id": "sLi-C8LvKSf4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 저장"
      ],
      "metadata": {
        "id": "Pg20pf8gKXiE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = \"beomi/Llama-3-Open-Ko-8B\"  # 병합을 수행할 베이스 모델\n",
        "huggingface_token = \"\"  # HuggingFace 토큰\n",
        "huggingface_repo = \"Llama-3-Open-Ko-8B-Instruct-teddynote\"  # 모델을 업로드할 repository\n",
        "save_method = (\n",
        "    \"merged_16bit\"  # \"merged_4bit\", \"merged_4bit_forced\", \"merged_16bit\", \"lora\"\n",
        ")"
      ],
      "metadata": {
        "id": "33__G4IcJ7tz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 옵션 1) 로컬에 저장"
      ],
      "metadata": {
        "id": "gIMbykndKbmG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained_merged(\n",
        "    base_model,\n",
        "    tokenizer,\n",
        "    save_method=save_method,  # 저장 방식을 16비트 병합으로 설정\n",
        ")"
      ],
      "metadata": {
        "id": "WbDA2epBKZsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 옵션 2) HuggingFace 에 업로드"
      ],
      "metadata": {
        "id": "WNun_4e-MVea"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+!! 허깅페이스 repository(write) 생성 필요"
      ],
      "metadata": {
        "id": "prG-uNXAYj47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi, HfFolder\n",
        "\n",
        "# Hugging Face 토큰 설정\n",
        "huggingface_token = \"개인키작성\"\n",
        "HfFolder.save_token(huggingface_token)\n",
        "\n",
        "# Hugging Face Hub API 객체 생성\n",
        "api = HfApi()\n",
        "\n",
        "# repo_id 확인\n",
        "repo_id = \"아이디/finetuning_test\"\n",
        "\n",
        "# 저장소가 존재하는지 확인, 존재하지 않으면 생성\n",
        "if not api.repo_exists(repo_id):\n",
        "    api.create_repo(repo_id)"
      ],
      "metadata": {
        "id": "LfrAl7hiRv8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hub 에 업로드\n",
        "model.push_to_hub_merged(\n",
        "    repo_id,\n",
        "    tokenizer,\n",
        "    save_method=save_method,\n",
        "    token=huggingface_token,\n",
        ")"
      ],
      "metadata": {
        "id": "I-A05lefKb9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GGUF 변환"
      ],
      "metadata": {
        "id": "180aRV_6cOqv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unsloth 는 llama.cpp를 복제하고 기본적으로 q8_0에 저장합니다. q4_k_m과 같은 모든 메소드를 사용할 수 있습니다. 로컬 저장을 위해서는 save_pretrained_gguf를 사용하고, HF에 업로드하기 위해서는 push_to_hub_gguf를 사용하세요."
      ],
      "metadata": {
        "id": "aLDwK_EhcRJY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[참고]\n",
        "\n",
        "* 개인 토큰은 https://huggingface.co/settings/tokens 에서 확인할 수 있습니다."
      ],
      "metadata": {
        "id": "TCDyFgFgcV4O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 옵션1) 로컬 저장"
      ],
      "metadata": {
        "id": "zC_8IHjOcYbn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantization 방식 설정\n",
        "quantization_method = \"q8_0\"  # \"f16\" \"q8_0\" \"q4_k_m\" \"q5_k_m\""
      ],
      "metadata": {
        "id": "GSVkYpbRcUSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "1InwnG9XcOcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained_gguf(\n",
        "    \"./content/drive/MyDrive/90_HuggingFace/Llama-3-Open-Ko-8B-Instruct-teddynote\",\n",
        "    tokenizer=tokenizer,\n",
        "    quantization_method=quantization_method,\n",
        ")"
      ],
      "metadata": {
        "id": "PHtxBAo4ccj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "용량 부족으로 수행 X"
      ],
      "metadata": {
        "id": "WR5ECuBsfZM0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 옵션2) HuggingFace 허브에 업로드"
      ],
      "metadata": {
        "id": "0pDP_wqgfbfq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "지원되는 몇 가지 양자화 방법들(전체 목록은 우리의 위키 페이지에서 확인 가능):\n",
        "\n",
        "* q8_0 - 빠른 변환. 높은 자원 사용이지만 일반적으로 수용 가능합니다.\n",
        "* q4_k_m - 추천됩니다. attention.wv와 feed_forward.w2 텐서의 절반에 Q6_K를 사용하고, 나머지는 Q4_K를 사용합니다.\n",
        "* q5_k_m - 추천됩니다. attention.wv와 feed_forward.w2 텐서의 절반에 Q6_K를 사용하고, 나머지는 Q5_K를 사용합니다."
      ],
      "metadata": {
        "id": "dmD2FA3-ffm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantization 방식 설정\n",
        "quantization_method = \"q8_0\"  # \"f16\" \"q8_0\" \"q4_k_m\" \"q5_k_m\""
      ],
      "metadata": {
        "id": "_b_tJMohcpnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# huggingface_repo = repo_id\n",
        "# Hub 에 GGUF 업로드\n",
        "model.push_to_hub_gguf(\n",
        "    repo_id + \"-gguf\",\n",
        "    tokenizer,\n",
        "    quantization_method=quantization_method,\n",
        "    token=huggingface_token,\n",
        ")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "QLbxFxJ7fa6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "용량 부족으로 수행 X"
      ],
      "metadata": {
        "id": "7saCsAwif6uC"
      }
    }
  ]
}